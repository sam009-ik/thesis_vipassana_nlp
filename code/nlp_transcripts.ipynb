{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cc6bd88-9222-4fc6-a6c5-611da7e5b119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alind.txt', 'anirudh.txt', 'anuradha.txt', 'fgd1.txt', 'fgd2.txt', 'himanshu.txt', 'himmat.txt', 'jyoti.txt', 'mani.txt', 'nandkishore.txt', 'premchand.txt', 'puneet.txt', 'ranjana.txt', 'saideep.txt', 'sandhya.txt', 'shaurya.txt', 'shveta.txt', 'sushila.txt', 'tonu.txt']\n"
     ]
    }
   ],
   "source": [
    "#Import Glob to read all text files\n",
    "import glob\n",
    "\n",
    "path = '*.txt'\n",
    "transcript_files = glob.glob(path)\n",
    "print(transcript_files) #There are 19 .txt files (17 interviews + 2 fgd's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c34b0103-dd8f-4bb7-964b-475b29bac11f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\samar\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\samar\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\samar\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\samar\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\samar\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: textblob in c:\\users\\samar\\anaconda3\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\samar\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\samar\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\samar\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\samar\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "#Install the necessary libraries\n",
    "!pip install nltk\n",
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36119f7-50f7-4f23-9ebf-eda8b7538cf2",
   "metadata": {},
   "source": [
    "The presence or absence of negative words like \"not\" and other negations can indeed affect the sentiment conveyed by the topics. If \"not\" or other negative words are included in the list of stop words, it could lead to a more positive-leaning interpretation of the topics, as negations often change the sentiment of a statement.\r\n",
    "\r\n",
    "Stop Words and Sentiment Bias:\r\n",
    "\r\n",
    "Stop words are commonly used words (such as \"the\", \"is\", \"in\", etc.) that are usually removed from texts before processing because they often don't contribute much meaningful information.\r\n",
    "However, words like \"not\" play a crucial role in the sentiment of a phrase. For instance, \"good\" and \"not good\" have opposite meanings. If negations are removed, the resulting topics might be skewed towards a more positive sentiment.\r\n",
    "Interpreting LDA Topics:\r\n",
    "\r\n",
    "When interpreting LDA topics, it's important to remember that LDA doesn't inherently understand the sentiment of words. It groups words based on their co-occurrence patterns.\r\n",
    "The interpretation of topics can be subjective, and without considering the context or negations, it may inadvertently lean towards a more positive or neutral portrayal.\r\n",
    "**Checking\r\n",
    "\r\n",
    "for Negation Words**:\r\n",
    "- To ensure a balanced interpretation, you might want to check if negation words like \"not\" are included in your stop words list. If they are, consider keeping them in your analysis.\r\n",
    "- Reviewing the list of stop words and adjusting it based on the context of your analysis is crucial. For a nuanced analysis, especially in sentiment-rich texts, it’s important to carefully choose which words are treated as stop words.\r\n",
    "\r\n",
    "Balanced Topic Interpretation:\r\n",
    "\r\n",
    "It’s beneficial to approach LDA topic interpretation with an awareness of potential biases. This includes considering both positive and negative aspects that might be present in the topics.\r\n",
    "Reinterpreting the topics with an eye for potential negations or contrasting sentiments could provide a more balanced view.\r\n",
    "Revisiting the LDA Model:\r\n",
    "\r\n",
    "If you suspect that the exclusion of negation words is affecting your results, you might want to rerun the LDA model without removing these words.\r\n",
    "Re-analyzing the topics with negations included could reveal different nuances in how Vipassana is discussed in your corpus.\r\n",
    "In summary, the treatment of negations and other sentiment-indic\r\n",
    "\r\n",
    "ative words in LDA preprocessing can significantly impact the interpretation of the model's output. It's always a good practice to critically evaluate your stop words list and consider the broader linguistic context to ensure a balanced and comprehensive analysis of the topics derived from your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4e0c68e6-37d4-4092-be90-e71ee4e1040b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\samar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['does', 'those', 'from', 'all', \"that'll\", 'what', 'won', 'these', 'who', 'wasn', 'is', 't', 'because', 'be', 'shouldn', 'aren', 'this', 'you', 'he', \"wasn't\", \"shouldn't\", 'down', 'again', 'few', 'them', 'so', 'until', 'ours', 'while', 'can', \"isn't\", 'doesn', \"couldn't\", 'a', 'too', 'doing', 'there', 'very', 'were', 'm', 'are', 'hasn', 're', 'but', \"hasn't\", 'don', \"you'll\", 'their', 'our', 'most', 'll', 'it', 'which', 'why', 'hadn', 'o', 'once', 'any', \"didn't\", 'under', 'through', 'just', 'about', 'when', 'she', 'if', 'didn', 'we', 'mightn', 'over', 'theirs', 'did', 'ain', 'do', 'for', 'into', 'further', 'how', 'having', 'to', 'being', \"hadn't\", 'on', 'more', 'where', 'myself', 'in', \"you've\", 'your', 'after', \"shan't\", 'mustn', 'haven', \"needn't\", 'himself', 'themselves', 'me', 'whom', 'weren', 'shan', 'his', 'they', 'should', 've', 'yourselves', 'couldn', 'has', 'during', 'will', \"you're\", 'itself', 'my', 'd', 'hers', 'ma', 'needn', 'of', 'that', \"haven't\", 'its', 'up', 'each', 'only', 's', \"you'd\", \"aren't\", 'or', 'below', \"she's\", 'out', 'between', 'y', 'then', 'was', 'been', 'above', 'other', \"should've\", \"it's\", 'had', 'same', 'off', 'isn', 'him', \"mightn't\", 'yourself', 'own', 'wouldn', 'by', 'an', 'the', 'than', 'at', 'some', 'and', \"mustn't\", 'before', 'both', 'have', 'as', \"weren't\", 'her', 'here', 'i', 'yours', 'with', 'am', 'herself', 'such', 'nt', 'anirudh', 'narula', 'like', 'yeah', 'ranjanaa', 'would', 'could', 'okay', 'vipassana']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "#dictionary to store tokenized data\n",
    "tokenized_data = {}\n",
    "\n",
    "#Loop over all transcript_files\n",
    "for txt_file in transcript_files:\n",
    "    with open(txt_file, \"r\", encoding='utf-8') as file:\n",
    "        transcript = file.read() #Read the .txt file insid the loop\n",
    "        \n",
    "    transcript = re.sub(r\"\\bInterviewer\\b: ?\", \"\", transcript, flags=re.IGNORECASE)\n",
    "    transcript = re.sub(r\"\\b(Alind|Anirudh|Anuradha|Himanshu|Himmat|Jyoti|Mani|Nandkishore|Premchand|Puneet|Ranjana|Saideep|Sandhya|Shaurya|Shveta|Sushila|Tonu|Ritu|Raunak|Richa)\\b: ?\", \"\", transcript, flags=re.IGNORECASE)\n",
    "    wtokenize = word_tokenize(transcript)\n",
    "    stokenize = sent_tokenize(transcript)\n",
    "    # Get the list of default English stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    #List of words which are mostly irrelevant\n",
    "    list_to_remove = ['no', 'nor', 'wouldn\\'t', 'against', 'won\\'t', 'ourselves', 'don\\'t', 'not', 'now', 'doesn\\'t']\n",
    "    stop_words = list(set(stop_words) - set(list_to_remove)) #Use set difference to remove elements from a list\n",
    "    list_to_add = ['nt', 'anirudh', 'narula', 'like', 'yeah', 'ranjanaa', 'would', 'could', 'okay', 'vipassana']\n",
    "    #print(stop_words)\n",
    "    stop_words = stop_words + list_to_add #simply add two lists like this\n",
    "    #We clean wtokenize, but punctuations still remain\n",
    "    # Convert the tokens to lowercase, remove punctuation, and then check against the stop words\n",
    "    clean_wtokenize = [re.sub(r\"[^a-zA-Z0-9 ]+\", \"\", word).lower() for word in wtokenize if re.sub(r\"[^a-zA-Z0-9 ]+\", \"\", word).lower() not in stop_words]\n",
    "    clean_stokenize = [re.sub(r\"[^a-zA-Z0-9 ]+\", \"\", word).lower() for word in stokenize if re.sub(r\"[^a-zA-Z0-9 ]+\", \"\", word).lower() not in stop_words]\n",
    "\n",
    "    # Filter out any empty strings that may have resulted from removing punctuation\n",
    "    clean_wtokenize = [word for word in clean_wtokenize if word]\n",
    "\n",
    "     # Apply stemming\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in clean_wtokenize]\n",
    "\n",
    "    # Sentiment analysis with TextBlob\n",
    "    blob = TextBlob(transcript)\n",
    "    sentiment = blob.sentiment\n",
    "    noun_phrases = blob.noun_phrases\n",
    "\n",
    "    word_freq = nltk.FreqDist(clean_wtokenize)\n",
    "\n",
    "    # Store tokenized data in the dictionary using filename as key\n",
    "    file_key = txt_file.split('.')[0]  # Extract filename from the path, remove .txt\n",
    "    tokenized_data[file_key] = {\n",
    "        'word_tokens': clean_wtokenize,\n",
    "        'sentence_tokens': clean_stokenize,\n",
    "        'stem_tokens': stemmed_tokens,\n",
    "        'sentiment': sentiment,\n",
    "        'noun_phrases': noun_phrases,\n",
    "        'word_freq': word_freq\n",
    "    }\n",
    "print(stop_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f23843e-1b6d-47df-b6fd-8b5ca4cddf59",
   "metadata": {},
   "source": [
    "corpora:\r\n",
    "\r\n",
    "In the context of Natural Language Processing (NLP), corpora is a term used to describe a large and structured set of texts.\r\n",
    "In your code, corpora likely refers to a module from the gensim library, which is used for unsupervised topic modeling and natural language processing.\r\n",
    "dictionary:\r\n",
    "\r\n",
    "Here, a dictionary is created using corpora.Dictionary(all_tokenized_texts). This is not a standard Python dictionary but a special object from the gensim library.\r\n",
    "The dictionary object maps each unique word in your text data to a unique integer ID. This process is crucial for converting text data into a numerical form that machine learning models can understand.\r\n",
    "corpus and Bag-of-Words (BoW):\r\n",
    "\r\n",
    "A corpus in NLP is a collection of documents, which in this case is a collection of your text data.\r\n",
    "doc2bow stands for \"document to Bag-of-Words\". It converts each document into the Bag-of-Words format, which is a list of tuples. Each tuple contains a word's ID (as per the dictionary) and its frequency in the document.\r\n",
    "Essentially, doc2bow converts your text data into a numerical form, where each document is represented as a vector of word frequencies.\r\n",
    "Parameters in LdaModel:\r\n",
    "\r\n",
    "corpus: This is the dataset you're using for training the model, in the BoW format.\r\n",
    "num_topics: The number of topics you want the LDA model to identify in your data.\r\n",
    "id2word: This maps IDs back to words. Here, it's the dictionary you created earlier.\r\n",
    "passes: The number of passes the algorithm makes over the entire corpus. More passes can lead to a more accurate model but also take longer to compute.\r\n",
    "random_state: This is a seed for random number generation, ensuring reproducibility of your results. Different seeds can lead to slightly different topic allocations.\r\n",
    "Determining the Dominant Topic:\r\n",
    "\r\n",
    "For each text file (txt_file), you create a BoW representation (bow).\r\n",
    "lda_model.get_document_topics(bow) gives you the topic distribution for that document.\r\n",
    "You find the dominant topic (the one with the highest proportion) for each document and store it along with the most significant words (and their weights) in that topic.\r\n",
    "dominant_topic_details:\r\n",
    "\r\n",
    "This dictionary stores the results. For each file, it keeps track of the dominant topic and the top words (with their respective weights) that characterize this topic.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e873b89c-6c9c-4351-9c73-1317db8fadd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.4276125088280711\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.009*\"life\" + 0.008*\"personal\" + 0.007*\"experience\" + 0.005*\"practice\" + 0.005*\"think\" + 0.005*\"understanding\" + 0.005*\"not\" + 0.005*\"challenges\" + 0.005*\"issues\" + 0.004*\"selfawareness\"'),\n",
       " (1,\n",
       "  '0.009*\"practice\" + 0.009*\"meditation\" + 0.007*\"nature\" + 0.006*\"often\" + 0.006*\"understanding\" + 0.005*\"law\" + 0.005*\"life\" + 0.005*\"experience\" + 0.005*\"personal\" + 0.005*\"approach\"'),\n",
       " (2,\n",
       "  '0.028*\"know\" + 0.021*\"not\" + 0.017*\"think\" + 0.016*\"people\" + 0.010*\"feel\" + 0.009*\"say\" + 0.008*\"one\" + 0.007*\"now\" + 0.007*\"right\" + 0.007*\"get\"'),\n",
       " (3,\n",
       "  '0.012*\"practice\" + 0.009*\"not\" + 0.007*\"sense\" + 0.007*\"understanding\" + 0.007*\"life\" + 0.006*\"might\" + 0.006*\"change\" + 0.006*\"self\" + 0.006*\"think\" + 0.005*\"others\"')]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "\n",
    "# Create a list of tokenized texts\n",
    "all_tokenized_texts = [data['word_tokens'] for data in tokenized_data.values()]\n",
    "\n",
    "# Create a dictionary and corpus for the LDA model\n",
    "dictionary = corpora.Dictionary(all_tokenized_texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in all_tokenized_texts]\n",
    "\n",
    "\n",
    "# Train the LDA model\n",
    "lda_model = LdaModel(corpus=corpus, num_topics=4, id2word=dictionary, passes=105, random_state=2, alpha='auto', eta='auto')\n",
    "\n",
    "dominant_topic_details = {}\n",
    "\n",
    "for txt_file, bow in zip(tokenized_data.keys(), corpus):\n",
    "    # Get the topic distribution for the document\n",
    "    topic_distribution = lda_model.get_document_topics(bow)\n",
    "    # Find the dominant topic (the one with the highest proportion)\n",
    "    dominant_topic = max(topic_distribution, key=lambda x: x[1])[0]\n",
    "    # Store the dominant topic for the file\n",
    "     # Get the word-weight pairs for the dominant topic\n",
    "    topic_words = lda_model.show_topic(dominant_topic, topn=8)\n",
    "    word_weight_pairs = [(word, round(weight, 3)) for word, weight in topic_words]\n",
    "\n",
    "    # Store the results\n",
    "    file_key = txt_file.split('.')[0]  # Extract filename\n",
    "    dominant_topic_details[file_key] = {\n",
    "        'dominant_topic': dominant_topic,\n",
    "        'word_weights': word_weight_pairs\n",
    "    }\n",
    "    \n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=all_tokenized_texts, dictionary=dictionary, corpus=corpus, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score:', coherence_lda)\n",
    "\n",
    "# Now, dominant_topics dictionary contains the dominant topic for each file\n",
    "\n",
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4d7b50-a7fe-4093-b46a-c08fbedeb972",
   "metadata": {},
   "source": [
    "Each topic offers a distinct perspective:\r\n",
    "\r\n",
    "Topic 0 - Integrative Life Perspectives:\r\n",
    "\r\n",
    "Keywords: \"life,\" \"personal,\" \"experience,\" \"practice,\" \"issues,\" \"societal,\" \"think,\" \"understanding,\" \"not,\" \"however.\"\r\n",
    "Analysis: This topic seems to encapsulate a holistic view of Vipassana, intertwining personal experiences and societal perspectives. It covers a broad range of elements from personal practice to societal issues, reflecting on the complexities and diverse interpretations of Vipassana in life. The presence of \"not\" and \"however\" suggests a nuanced or possibly critical exploration of these aspects.\r\n",
    "Suggested Name: \"Vipassana: Personal and Societal Dynamics\"\r\n",
    "Topic 1 - Meditative Practices and Philosophies:\r\n",
    "\r\n",
    "Keywords: \"practice,\" \"meditation,\" \"nature,\" \"often,\" \"understanding,\" \"law,\" \"life,\" \"experience,\" \"personal,\" \"approach.\"\r\n",
    "Analysis: This topic is likely centered around the meditation practice itself, including its nature and various philosophical understandings. It might discuss the regularity (\"often\"), principles (\"law\"), and different approaches to meditation, emphasizing how these aspects are woven into personal life and experiences.\r\n",
    "Suggested Name: \"Philosophical Dimensions of Meditation\"\r\n",
    "Topic 2 - Critical Reflection and Discourse:\r\n",
    "\r\n",
    "Keywords: \"know,\" \"not,\" \"think,\" \"people,\" \"feel,\" \"say,\" \"one,\" \"now,\" \"right,\" \"something.\"\r\n",
    "Analysis: Dominated by words like \"know,\" \"not,\" and \"think,\" this topic seems to represent a space of critical reflection and discourse. It suggests discussions where ideas, beliefs, and feelings about Vipassana are questioned, debated, and reflected upon, possibly indicating diverse and contrasting opinions.\r\n",
    "Suggested Name: \"Debating the Essence of Vipassana\"\r\n",
    "Topic 3 - Self-Awareness and Transformation:\r\n",
    "\r\n",
    "Keywords: \"practice,\" \"not,\" \"sense,\" \"understanding,\" \"life,\" \"might,\" \"think,\" \"change,\" \"self,\" \"others.\"\r\n",
    "Analysis: This topic appears to focus on the journey of self-awareness and potential transformation through Vipassana. It covers aspects of personal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "49a52f90-5738-48c0-b803-8144c39a573c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_individual_details(name, tokenized_data, dominant_topic_details):\n",
    "    # Check if the individual's data exists\n",
    "    if name not in tokenized_data:\n",
    "        print(f\"No data found for {name}.\")\n",
    "        return\n",
    "    #The basic syntax is dictionary.get(key, default).\n",
    "    #key: This is the key for which you want to retrieve the value.\n",
    "    #default (optional): This is the value that will be returned if the key does not exist in the dictionary. If you don't provide a default value, it will return None by default.\n",
    "    # Extracting individual's data\n",
    "    individual_data = tokenized_data[name]\n",
    "    individual_tokens = individual_data.get('word_tokens', [])\n",
    "    individual_stemmed = individual_data.get('stem_tokens', [])\n",
    "    individual_sentiment = individual_data.get('sentiment', None)\n",
    "    individual_nphrases = individual_data.get('noun_phrases', [])\n",
    "    individual_word_freq = individual_data.get('word_freq', None)\n",
    "\n",
    "    # Print basic information\n",
    "    print(f\"Details for {name.capitalize()}:\")\n",
    "    print(f\"First 10 Word Tokens: {individual_tokens[:10]}\")\n",
    "    print(f\"First 10 Stemmed Tokens: {individual_stemmed[:10]}\")\n",
    "    print(f\"Sentiment: {individual_sentiment}\")\n",
    "    print(f\"First 10 Noun Phrases: {individual_nphrases[:10]}\")\n",
    "    if individual_word_freq:\n",
    "        print(f\"Top 10 Word Frequencies: {individual_word_freq.most_common(10)}\")\n",
    "    else:\n",
    "        print(\"Word frequency data not available.\")\n",
    "\n",
    "    # Get the dominant topic\n",
    "    individual_details = dominant_topic_details.get(name, {})\n",
    "    dominant_topic = individual_details.get('dominant_topic', 'Not available')\n",
    "    word_weight_pairs = individual_details.get('word_weights', [])\n",
    "\n",
    "    print(f\"Dominant Topic: {dominant_topic}\")\n",
    "    print(\"Word-Weight Pairs:\")\n",
    "    for word, weight in word_weight_pairs:\n",
    "        print(f\"{word}, {weight}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "701c2e2b-53a4-4970-a0cf-ea0f6eb6c2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Obtaining dependency information for spacy from https://files.pythonhosted.org/packages/90/f0/0133b684e18932c7bf4075d94819746cee2c0329f2569db526b0fa1df1df/spacy-3.7.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading spacy-3.7.2-cp311-cp311-win_amd64.whl.metadata (26 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Obtaining dependency information for spacy-loggers<2.0.0,>=1.0.0 from https://files.pythonhosted.org/packages/33/78/d1a1a026ef3af911159398c939b1509d5c36fe524c7b644f34a5146c4e16/spacy_loggers-1.0.5-py3-none-any.whl.metadata\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Obtaining dependency information for murmurhash<1.1.0,>=0.28.0 from https://files.pythonhosted.org/packages/71/46/af01a20ec368bd9cb49a1d2df15e3eca113bbf6952cc1f2a47f1c6801a7f/murmurhash-1.0.10-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading murmurhash-1.0.10-cp311-cp311-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Obtaining dependency information for cymem<2.1.0,>=2.0.2 from https://files.pythonhosted.org/packages/c1/c3/dd044e6f62a3d317c461f6f0c153c6573ed13025752d779e514000c15dd2/cymem-2.0.8-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading cymem-2.0.8-cp311-cp311-win_amd64.whl.metadata (8.6 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Obtaining dependency information for preshed<3.1.0,>=3.0.2 from https://files.pythonhosted.org/packages/e4/fc/78cdbdb79f5d6d45949e72c32445d6c060977ad50a1dcfc0392622165f7c/preshed-3.0.9-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.1.8 (from spacy)\n",
      "  Obtaining dependency information for thinc<8.3.0,>=8.1.8 from https://files.pythonhosted.org/packages/74/24/564a7df5b1fac0520f6b55137deea2cc0b6f7d6e66228f1645dbfd59bb33/thinc-8.2.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading thinc-8.2.2-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Obtaining dependency information for wasabi<1.2.0,>=0.9.1 from https://files.pythonhosted.org/packages/8f/69/26cbf0bad11703241cb84d5324d868097f7a8faf2f1888354dac8883f3fc/wasabi-1.1.2-py3-none-any.whl.metadata\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Obtaining dependency information for srsly<3.0.0,>=2.4.3 from https://files.pythonhosted.org/packages/eb/f5/e3f29993f673d91623df6413ba64e815dd2676fd7932cbc5e7347402ddae/srsly-2.4.8-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading srsly-2.4.8-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Obtaining dependency information for catalogue<2.1.0,>=2.0.6 from https://files.pythonhosted.org/packages/9e/96/d32b941a501ab566a16358d68b6eb4e4acc373fab3c3c4d7d9e649f7b4bb/catalogue-2.0.10-py3-none-any.whl.metadata\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n",
      "  Obtaining dependency information for weasel<0.4.0,>=0.1.0 from https://files.pythonhosted.org/packages/d5/e5/b63b8e255d89ba4155972990d42523251d4d1368c4906c646597f63870e2/weasel-0.3.4-py3-none-any.whl.metadata\n",
      "  Downloading weasel-0.3.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "     ---------------------------------------- 0.0/45.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 45.9/45.9 kB ? eta 0:00:00\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from spacy) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\samar\\anaconda3\\lib\\site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from spacy) (23.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "     ---------------------------------------- 0.0/181.6 kB ? eta -:--:--\n",
      "     ------------------------------------- 181.6/181.6 kB 11.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from spacy) (1.26.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.1.8->spacy)\n",
      "  Obtaining dependency information for blis<0.8.0,>=0.7.8 from https://files.pythonhosted.org/packages/2f/09/da0592c74560cc33396504698122f7a56747c82a5e072ca7d2c3397898e1/blis-0.7.11-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading blis-0.7.11-cp311-cp311-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.1.8->spacy)\n",
      "  Obtaining dependency information for confection<1.0.0,>=0.0.1 from https://files.pythonhosted.org/packages/39/78/f9d18da7b979a2e6007bfcea2f3c8cc02ed210538ae1ce7e69092aed7b18/confection-0.1.4-py3-none-any.whl.metadata\n",
      "  Downloading confection-0.1.4-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\samar\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n",
      "  Obtaining dependency information for cloudpathlib<0.17.0,>=0.7.0 from https://files.pythonhosted.org/packages/0f/6e/45b57a7d4573d85d0b0a39d99673dc1f5eea9d92a1a4603b35e968fbf89a/cloudpathlib-0.16.0-py3-none-any.whl.metadata\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Downloading spacy-3.7.2-cp311-cp311-win_amd64.whl (12.1 MB)\n",
      "   ---------------------------------------- 0.0/12.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/12.1 MB 10.6 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.8/12.1 MB 10.0 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 1.2/12.1 MB 10.8 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.7/12.1 MB 10.1 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.8/12.1 MB 9.5 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.1/12.1 MB 8.0 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.3/12.1 MB 7.4 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.7/12.1 MB 7.5 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 3.1/12.1 MB 7.6 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 3.5/12.1 MB 7.8 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 4.1/12.1 MB 8.1 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 4.4/12.1 MB 8.3 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.9/12.1 MB 8.2 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 5.3/12.1 MB 8.3 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.6/12.1 MB 8.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 5.9/12.1 MB 8.0 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.4/12.1 MB 8.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 7.0/12.1 MB 8.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.4/12.1 MB 8.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.6/12.1 MB 8.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 8.0/12.1 MB 8.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 8.3/12.1 MB 8.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.8/12.1 MB 8.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.3/12.1 MB 8.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.8/12.1 MB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.3/12.1 MB 8.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.9/12.1 MB 8.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.4/12.1 MB 8.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.1 MB 9.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.1/12.1 MB 9.0 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.8-cp311-cp311-win_amd64.whl (39 kB)\n",
      "Downloading murmurhash-1.0.10-cp311-cp311-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl (122 kB)\n",
      "   ---------------------------------------- 0.0/122.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 122.3/122.3 kB 7.5 MB/s eta 0:00:00\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp311-cp311-win_amd64.whl (479 kB)\n",
      "   ---------------------------------------- 0.0/479.7 kB ? eta -:--:--\n",
      "   --------------------------------------  471.0/479.7 kB 14.9 MB/s eta 0:00:01\n",
      "   --------------------------------------- 479.7/479.7 kB 15.1 MB/s eta 0:00:00\n",
      "Downloading thinc-8.2.2-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 0.5/1.5 MB 13.8 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.6/1.5 MB 8.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.1/1.5 MB 9.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 9.4 MB/s eta 0:00:00\n",
      "Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.1/50.1 kB ? eta 0:00:00\n",
      "Downloading blis-0.7.11-cp311-cp311-win_amd64.whl (6.6 MB)\n",
      "   ---------------------------------------- 0.0/6.6 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/6.6 MB 14.2 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 1.1/6.6 MB 11.3 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 1.6/6.6 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 2.2/6.6 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 2.8/6.6 MB 11.9 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 3.3/6.6 MB 11.6 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.8/6.6 MB 11.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.4/6.6 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 5.0/6.6 MB 11.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.5/6.6 MB 11.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.0/6.6 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.6/6.6 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.6/6.6 MB 11.1 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/45.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 45.0/45.0 kB ? eta 0:00:00\n",
      "Downloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, murmurhash, langcodes, cloudpathlib, catalogue, blis, typer, srsly, preshed, confection, weasel, thinc, spacy\n",
      "Successfully installed blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 confection-0.1.4 cymem-2.0.8 langcodes-3.3.0 murmurhash-1.0.10 preshed-3.0.9 spacy-3.7.2 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.2 typer-0.9.0 wasabi-1.1.2 weasel-0.3.4\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 495.5 kB/s eta 0:00:26\n",
      "      --------------------------------------- 0.2/12.8 MB 1.5 MB/s eta 0:00:09\n",
      "     -- ------------------------------------- 0.8/12.8 MB 4.4 MB/s eta 0:00:03\n",
      "     ---- ----------------------------------- 1.4/12.8 MB 6.7 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 7.3 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 2.5/12.8 MB 8.2 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 3.1/12.8 MB 9.0 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 3.6/12.8 MB 9.3 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 4.2/12.8 MB 9.6 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 4.8/12.8 MB 9.9 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 5.4/12.8 MB 10.1 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 6.0/12.8 MB 10.1 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 6.5/12.8 MB 10.4 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 7.0/12.8 MB 10.4 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 7.6/12.8 MB 10.6 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 8.2/12.8 MB 10.7 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.7/12.8 MB 10.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 9.1/12.8 MB 10.4 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.8/12.8 MB 10.8 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 10.5/12.8 MB 12.4 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 11.1/12.8 MB 12.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 11.6/12.8 MB 12.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 12.1/12.8 MB 12.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.6/12.8 MB 11.9 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.8 MB 12.1 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 11.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\samar\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.11.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\samar\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ae0e9f72-76f6-48af-81d7-f3d224992437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_tag</th>\n",
       "      <th>ORG</th>\n",
       "      <th>DATE</th>\n",
       "      <th>PERSON</th>\n",
       "      <th>CARDINAL</th>\n",
       "      <th>GPE</th>\n",
       "      <th>ORDINAL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alind</td>\n",
       "      <td>14</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anuradha</td>\n",
       "      <td>14</td>\n",
       "      <td>14.0</td>\n",
       "      <td>7</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jyoti</td>\n",
       "      <td>14</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>saideep</td>\n",
       "      <td>28</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sandhya</td>\n",
       "      <td>15</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>shveta</td>\n",
       "      <td>10</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sushila</td>\n",
       "      <td>21</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tonu</td>\n",
       "      <td>26</td>\n",
       "      <td>51.0</td>\n",
       "      <td>16</td>\n",
       "      <td>21.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>anirudh</td>\n",
       "      <td>28</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fgd1</td>\n",
       "      <td>29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>fgd2</td>\n",
       "      <td>33</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>himanshu</td>\n",
       "      <td>15</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>himmat</td>\n",
       "      <td>24</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mani</td>\n",
       "      <td>20</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>nandkishore</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>premchand</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>puneet</td>\n",
       "      <td>31</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ranjana</td>\n",
       "      <td>23</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>shaurya</td>\n",
       "      <td>33</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     person_tag  ORG  DATE  PERSON  CARDINAL   GPE  ORDINAL\n",
       "0         alind   14  10.0       9       4.0   2.0      1.0\n",
       "1      anuradha   14  14.0       7      13.0   1.0      6.0\n",
       "2         jyoti   14   2.0       7       1.0   1.0      2.0\n",
       "3       saideep   28   3.0       7       2.0   2.0      1.0\n",
       "4       sandhya   15   4.0       6       1.0   0.0      1.0\n",
       "5        shveta   10  18.0       6       4.0   1.0      5.0\n",
       "6       sushila   21   2.0       5       2.0   0.0      1.0\n",
       "7          tonu   26  51.0      16      21.0  11.0      4.0\n",
       "8       anirudh   28   4.0       9       0.0   0.0      0.0\n",
       "9          fgd1   29   0.0      10       0.0   1.0      0.0\n",
       "10         fgd2   33   3.0      10       1.0   3.0      0.0\n",
       "11     himanshu   15   2.0       9       1.0   0.0      0.0\n",
       "12       himmat   24   7.0       7       3.0   1.0      0.0\n",
       "13         mani   20   3.0       5       0.0   0.0      0.0\n",
       "14  nandkishore    3   0.0       2       0.0   0.0      0.0\n",
       "15    premchand   12   0.0       5       0.0   0.0      0.0\n",
       "16       puneet   31   1.0       5       2.0   0.0      0.0\n",
       "17      ranjana   23   3.0      11       1.0   0.0      0.0\n",
       "18      shaurya   33   2.0      14       0.0   0.0      0.0"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "ner_details = {}\n",
    "all_counters = {}\n",
    "#Loop over all transcript_files\n",
    "for txt_file in transcript_files:\n",
    "    with open(txt_file, \"r\", encoding='utf-8') as file:\n",
    "        transcript = file.read() #Read the .txt file insid the loop\n",
    "    \n",
    "    transcript = re.sub(r\"\\bInterviewer\\b: ?\", \"\", transcript, flags=re.IGNORECASE)\n",
    "    transcript = re.sub(r\"\\b(Alind|Anirudh|Anuradha|Himanshu|Himmat|Jyoti|Mani|Nandkishore|Premchand|Puneet|Ranjana|Saideep|Sandhya|Shaurya|Shveta|Sushila|Tonu|Ritu|Raunak|Richa)\\b: ?\", \"\", transcript, flags=re.IGNORECASE)\n",
    "    doc = nlp(transcript)\n",
    "    # Extract named entities\n",
    "    named_entities = []\n",
    "    for ent in doc.ents:\n",
    "        context = ' '.join([token.text for token in ent.sent]) # Extracting sentence where the entity is mentioned\n",
    "        named_entities.append({\n",
    "            'text': ent.text,\n",
    "            'type': ent.label_,\n",
    "            'context': context\n",
    "        })\n",
    "        \n",
    "    entity_freq = Counter([ent.label_ for ent in doc.ents])\n",
    "    \n",
    "    file_key = txt_file.split(\".\")[0]\n",
    "    ner_details[file_key] = {\n",
    "        'named_entities': named_entities,\n",
    "        'entity_freq': entity_freq\n",
    "    }\n",
    "      # Add entity frequency to all_counters\n",
    "    all_counters[file_key] = entity_freq\n",
    "\n",
    "# Convert all_counters to DataFrame\n",
    "df_counters = pd.DataFrame.from_dict(all_counters, orient='index').fillna(0)\n",
    "df_counters.reset_index(inplace=True)\n",
    "df_counters.rename(columns={'index': 'person_tag'}, inplace=True)\n",
    "df_counters = df_counters[['person_tag', 'ORG', 'DATE', 'PERSON', 'CARDINAL', 'GPE', 'ORDINAL']]\n",
    "df_counters.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ba2beab9-878e-4c2d-a410-6b2c074a893f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details for Alind:\n",
      "First 10 Word Tokens: ['see', 'change', 'say', 'noticeable', 'change', 'no', 'surface', 'level', 'changes', 'change']\n",
      "First 10 Stemmed Tokens: ['see', 'chang', 'say', 'notic', 'chang', 'no', 'surfac', 'level', 'chang', 'chang']\n",
      "Sentiment: Sentiment(polarity=0.1487823984526112, subjectivity=0.44960976021614324)\n",
      "First 10 Noun Phrases: ['wouldn ’ t', 'surface level changes', 'transient nature', 'vipassana', 'vipassana', 'was', 'okay', 'was', 'surface level', 'non vegetarian food']\n",
      "Top 10 Word Frequencies: [('people', 31), ('not', 29), ('think', 29), ('know', 26), ('say', 24), ('feel', 16), ('change', 15), ('sense', 15), ('no', 14), ('course', 12)]\n",
      "Dominant Topic: 2\n",
      "Word-Weight Pairs:\n",
      "know, 0.02800000086426735\n",
      "not, 0.020999999716877937\n",
      "think, 0.017000000923871994\n",
      "people, 0.01600000075995922\n",
      "feel, 0.009999999776482582\n",
      "say, 0.008999999612569809\n",
      "one, 0.00800000037997961\n",
      "now, 0.007000000216066837\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'ORG': 14,\n",
       "         'DATE': 10,\n",
       "         'PERSON': 9,\n",
       "         'CARDINAL': 4,\n",
       "         'GPE': 2,\n",
       "         'ORDINAL': 1})"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#id 1 Alind\n",
    "get_individual_details(\"alind\", tokenized_data, dominant_topic_details)\n",
    "ner_details['alind'].get('named_entities')\n",
    "ner_details['alind'].get('entity_freq')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3539d050-2175-4a0c-9af8-5aa7995cf626",
   "metadata": {},
   "source": [
    "#### Overall Word Weights in Topics:\r\n",
    "\r\n",
    "The word weights you initially see in the LDA model's output (for each of the four topics) are calculated based on the entire corpus. They represent how important each word is for a given topic across all documents in your dataset.\r\n",
    "Individual Document (or Individual) Specific Word Weights:\r\n",
    "\r\n",
    "When you find the dominant topic for an individual (like \"alind\" in your example), the word weights are specific to how that topic is represented in that individual's text.\r\n",
    "This means that while \"alind\" might have been classified under \"Topic 2: Debating the Essence of Vipassana,\" the specific word weights (like \"know: 0.028\" and \"not: 0.021\") are unique to their document. These weights indicate the relative importance of each word within the context of \"alind's\" text in relation to the identified dominant topic.\r\n",
    "Interpreting Individual-Specific Word Weights:\r\n",
    "\r\n",
    "These individual-specific word weights can provide insights into how the themes or topics manifest in different individuals' texts.\r\n",
    "For example, if \"know\" and \"not\" have high weights in \"alind's\" dominant topic, it suggests that his/her discussion or thinking around the topic involves a significant amount of knowledge and negation, which is central to his/her understanding or perspective on \n",
    "\n",
    "#### The Counter object you're showing represents the frequency of different types of named entities in Alind's text. Here's what each of the entity labels typically represents in spaCy's NER model:\r\n",
    "\r\n",
    "ORG: An organization (e.g., Google, United Nations)\r\n",
    "DATE: A date (e.g., July 4th, 2021)\r\n",
    "PERSON: A person's name (e.g., John Doe)\r\n",
    "CARDINAL: Numerals that do not fall under another type (e.g., one, two, 1,000)\r\n",
    "GPE: Geopolitical entity, i.e., countries, cities, states (e.g., India, Paris)\r\n",
    "ORDINAL: \"first\", \"second\", etc.\r\n",
    "Interpreting this data depends on the context of your dissertation. Here's how it might be relevant:\r\n",
    "\r\n",
    "Organizational Context: If ORG entities are the most frequent in Alind's text, this might indicate a strong discussion about various organizations, which could be relevant if your dissertation discusses how individuals relate to or are influenced by organizations.\r\n",
    "\r\n",
    "Temporal Aspects: The frequency of DATE entities suggests that specific points in time or periods are significant in the transcript. This could be relevant if the temporal context is important to your analysis.\r\n",
    "\r\n",
    "Personal Interactions: The presence of PERSON entities might show that Alind's text involves many references to individuals, which could be important if your dissertation involves the social networks or interactions between people.\r\n",
    "\r\n",
    "Quantitative Analysis: CARDINAL and ORDINAL entities indicate the presence of\r\n",
    "\r\n",
    "numerical and sequential data. If your dissertation deals with quantifying aspects of Vipassana or ranking/ordering of concepts or experiences, these could be pertinent.\r\n",
    "\r\n",
    "Geographical Relevance: GPE entities can show the geographical diversity in Alind's text. If your dissertation explores the cultural or regional aspects of Vipassana practice, these references could be quite relevant.Vipassana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "71d8cdbf-506d-45fa-85bf-a36dc4a81f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details for Saideep:\n",
      "First 10 Word Tokens: ['perceive', 'concept', 'fraternity', 'relation', 'really', 'group', 'dynamics', 'fraternity', 'focused', 'introspection']\n",
      "First 10 Stemmed Tokens: ['perceiv', 'concept', 'fratern', 'relat', 'realli', 'group', 'dynam', 'fratern', 'focus', 'introspect']\n",
      "Sentiment: Sentiment(polarity=0.11742166563595129, subjectivity=0.482493300350443)\n",
      "First 10 Noun Phrases: ['vipassana', 'vipassana', 'group dynamics', 'individual inner journeys', 'collective identity', 'sadhguru', 'concept relate', 'vipassana', 'sadhguru', 'vipassana']\n",
      "Top 10 Word Frequencies: [('might', 7), ('people', 5), ('conflict', 5), ('concept', 4), ('experience', 4), ('see', 4), ('life', 4), ('challenging', 4), ('fraternity', 3), ('understanding', 3)]\n",
      "Dominant Topic: 3\n",
      "Word-Weight Pairs:\n",
      "practice, 0.012000000104308128\n",
      "not, 0.008999999612569809\n",
      "sense, 0.007000000216066837\n",
      "understanding, 0.007000000216066837\n",
      "life, 0.007000000216066837\n",
      "might, 0.006000000052154064\n",
      "change, 0.006000000052154064\n",
      "self, 0.006000000052154064\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#id 2 Saideep\n",
    "get_individual_details(\"saideep\", tokenized_data, dominant_topic_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24acef9-c4c9-4b5e-90c8-a246f4a1d79d",
   "metadata": {},
   "source": [
    "The words in the word-weight pairs for each individual corresponding to their dominant topic being the same as those in the overall topic descriptions is not by chance. This consistency is due to how LDA (Latent Dirichlet Allocation) models work. Here's a more detailed explanation:\r\n",
    "\r\n",
    "LDA Topic Composition:\r\n",
    "\r\n",
    "An LDA model identifies a fixed number of topics from the entire corpus. Each topic is characterized by a distribution of words. For instance, in your model, Topic 2 might be characterized by words like \"know,\" \"not,\" \"think,\" etc., based on their frequency and distribution across all documents.\r\n",
    "Document (or Individual) Assignment to Topics:\r\n",
    "\r\n",
    "When the LDA model analyzes an individual document (or in your case, an individual's text), it doesn't create new topics specific to that document. Instead, it tries to represent the document as a mixture of the topics it has already learned from the entire corpus.\r\n",
    "The model assigns a document to topics based on how well the words in the document align with the words that are significant in each topic.\r\n",
    "Word-Weight Pairs in Individual Analysis:\r\n",
    "\r\n",
    "For each individual, the model calculates the proportion of each topic present in their text. The \"dominant topic\" is the one with the highest proportion.\r\n",
    "The word-weight pairs for an individual in their dominant topic are derived from the overall word distribution of that topic. However, the specific weights are unique to the individual, indicating the relative importance of these words in the context of their text.\r\n",
    "Why Words Match the Overall Topic Words:\r\n",
    "\r\n",
    "The reason you see the same words (like \"life,\" \"personal,\" \"experience\" for Anirudh's dominant topic 0) is that these words are key to defining Topic 0 in the overall corpus.\r\n",
    "The model is essentially saying, \"Based on the presence and weight of these particular words in Anirudh\r\n",
    "'s text, his discussion aligns most closely with Topic 0, which is characterized by these words in the entire dataset.\"\r\n",
    "\r\n",
    "Individualized Word Weights:\r\n",
    "The actual weight values for Anirudh (or any other individual) show how prominent these words are in their specific text, compared to their importance in defining the topic across the entire corpus.\r\n",
    "So, while \"life,\" \"personal,\" \"experience,\" etc.,\r\n",
    "are significant words for Topic 0 in general, their specific weights in Anirudh's text tell you how much these particular words contribute to making Topic 0 the dominant topic for him.\r\n",
    "\r\n",
    "In summary, the words in the word-weight pairs for each individual are the same as those in the overall topic descriptions because they are key words that define each topic. However, the specific weights of these words vary from individual to individual, reflecting how these words are used in the context of each person's text and how strongly they associate with the dominant topic in that specific context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b843d444-dcf9-47b6-885b-90aa51ffbadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details for Shaurya:\n",
      "First 10 Word Tokens: ['good', 'morning', 'shaurya', 'law', 'student', 'aspiring', 'ips', 'officer', 'drew', 'single']\n",
      "First 10 Stemmed Tokens: ['good', 'morn', 'shaurya', 'law', 'student', 'aspir', 'ip', 'offic', 'drew', 'singl']\n",
      "Sentiment: Sentiment(polarity=0.10873482726423898, subjectivity=0.4491433239962653)\n",
      "First 10 Noun Phrases: ['good morning', 'shaurya', 'law student', 'ips', 'vipassana', 'good morning', 'vipassana', 'academic interest', 'buddha', 'ambedkar']\n",
      "Top 10 Word Frequencies: [('law', 13), ('public', 10), ('service', 10), ('nature', 8), ('ips', 7), ('officer', 7), ('experience', 7), ('career', 7), ('practice', 7), ('life', 7)]\n",
      "Dominant Topic: 1\n",
      "Word-Weight Pairs:\n",
      "practice, 0.008999999612569809\n",
      "meditation, 0.008999999612569809\n",
      "nature, 0.007000000216066837\n",
      "often, 0.006000000052154064\n",
      "understanding, 0.006000000052154064\n",
      "law, 0.004999999888241291\n",
      "life, 0.004999999888241291\n",
      "experience, 0.004999999888241291\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#id 3 Shaurya\n",
    "get_individual_details(\"shaurya\", tokenized_data, dominant_topic_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "61079e19-1aaf-4584-9ce7-775928f5d682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details for Mani:\n",
      "First 10 Word Tokens: ['tell', 'experiences', 'many', 'times', 'attended', 'attended', 'threeday', 'course', 'last', 'one']\n",
      "First 10 Stemmed Tokens: ['tell', 'experi', 'mani', 'time', 'attend', 'attend', 'threeday', 'cours', 'last', 'one']\n",
      "Sentiment: Sentiment(polarity=0.15221417069243154, subjectivity=0.42913503565677474)\n",
      "First 10 Noun Phrases: ['vipassana', 'vipassana', 'november', 'vipassana', 'mental peace', 'certain life', \"life 's questions\", 'vipassana', 'post-vipassana', 'stress levels']\n",
      "Top 10 Word Frequencies: [('understanding', 5), ('meditation', 5), ('social', 5), ('life', 4), ('practice', 3), ('well', 3), ('not', 3), ('day', 3), ('teaches', 3), ('without', 3)]\n",
      "Dominant Topic: 3\n",
      "Word-Weight Pairs:\n",
      "practice, 0.012000000104308128\n",
      "not, 0.008999999612569809\n",
      "sense, 0.007000000216066837\n",
      "understanding, 0.007000000216066837\n",
      "life, 0.007000000216066837\n",
      "might, 0.006000000052154064\n",
      "change, 0.006000000052154064\n",
      "self, 0.006000000052154064\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#id 4 Mani\n",
    "get_individual_details(\"mani\", tokenized_data, dominant_topic_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e80cf01a-55bd-4c9f-853f-b1060e1b9fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details for Himanshu:\n",
      "First 10 Word Tokens: ['think', 'general', 'meditation', 'foster', 'sense', 'fraternity', 'since', 'attended', 'perspective', 'limited']\n",
      "First 10 Stemmed Tokens: ['think', 'gener', 'medit', 'foster', 'sens', 'fratern', 'sinc', 'attend', 'perspect', 'limit']\n",
      "Sentiment: Sentiment(polarity=0.1336591538972491, subjectivity=0.49066229185276816)\n",
      "First 10 Noun Phrases: ['vipassana', 'general meditation', 'vipassana', 'individual journey', 'vipassana', 'self-perception post-vipassana', 'vipassana', \"n't facilitate making bonds\", 'vibration aspects', 'vibrations part']\n",
      "Top 10 Word Frequencies: [('practice', 7), ('sense', 6), ('think', 5), ('meditation', 5), ('course', 4), ('might', 4), ('experience', 3), ('seems', 3), ('individual', 3), ('changes', 3)]\n",
      "Dominant Topic: 3\n",
      "Word-Weight Pairs:\n",
      "practice, 0.012000000104308128\n",
      "not, 0.008999999612569809\n",
      "sense, 0.007000000216066837\n",
      "understanding, 0.007000000216066837\n",
      "life, 0.007000000216066837\n",
      "might, 0.006000000052154064\n",
      "change, 0.006000000052154064\n",
      "self, 0.006000000052154064\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#id 5 Himanshu\n",
    "get_individual_details(\"himanshu\", tokenized_data, dominant_topic_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b2d542cc-84e6-4c49-a664-252dcdefb76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details for Sandhya:\n",
      "First 10 Word Tokens: ['good', 'morning', 'sandhya', 'share', 'us', 'experience', 'many', 'times', 'attended', 'good']\n",
      "First 10 Stemmed Tokens: ['good', 'morn', 'sandhya', 'share', 'us', 'experi', 'mani', 'time', 'attend', 'good']\n",
      "Sentiment: Sentiment(polarity=0.2220714285714286, subjectivity=0.46523809523809534)\n",
      "First 10 Noun Phrases: ['good morning', 'sandhya', 'vipassana', 'good morning', 'vipassana', 'vipassana', 'vipassana', 'inner peace', 'professional crossroads', 'vipassana']\n",
      "Top 10 Word Frequencies: [('understanding', 5), ('not', 5), ('practice', 4), ('life', 4), ('awareness', 4), ('taught', 3), ('thoughts', 3), ('emotions', 3), ('good', 2), ('morning', 2)]\n",
      "Dominant Topic: 3\n",
      "Word-Weight Pairs:\n",
      "practice, 0.012000000104308128\n",
      "not, 0.008999999612569809\n",
      "sense, 0.007000000216066837\n",
      "understanding, 0.007000000216066837\n",
      "life, 0.007000000216066837\n",
      "might, 0.006000000052154064\n",
      "change, 0.006000000052154064\n",
      "self, 0.006000000052154064\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#id 6 Sandhya\n",
    "get_individual_details(\"sandhya\", tokenized_data, dominant_topic_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e6d5730f-01ce-4a76-a2bf-ced700e97adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details for Anuradha:\n",
      "First 10 Word Tokens: ['basically', 'dissertation', 'topic', 'fraternity', 'context', 'context', 'fraternity', 'know', 'question', 'mark']\n",
      "First 10 Stemmed Tokens: ['basic', 'dissert', 'topic', 'fratern', 'context', 'context', 'fratern', 'know', 'question', 'mark']\n",
      "Sentiment: Sentiment(polarity=0.1346595522356944, subjectivity=0.5281511776435632)\n",
      "First 10 Noun Phrases: ['okay', 'dissertation topic', 'vipassana', 'vipassana', 'question mark', 'vipassana', 'generate fraternity', 'okay', 'vipassana', 'november']\n",
      "Top 10 Word Frequencies: [('know', 37), ('think', 26), ('not', 24), ('one', 18), ('sense', 17), ('something', 14), ('get', 14), ('also', 12), ('religion', 12), ('no', 11)]\n",
      "Dominant Topic: 2\n",
      "Word-Weight Pairs:\n",
      "know, 0.02800000086426735\n",
      "not, 0.020999999716877937\n",
      "think, 0.017000000923871994\n",
      "people, 0.01600000075995922\n",
      "feel, 0.009999999776482582\n",
      "say, 0.00800000037997961\n",
      "one, 0.00800000037997961\n",
      "now, 0.007000000216066837\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#id 7 Anuradha\n",
    "get_individual_details(\"anuradha\", tokenized_data, dominant_topic_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a3bbcb8d-7688-440e-868c-8e078baad5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details for Shveta:\n",
      "First 10 Word Tokens: ['hi', 'good', 'tell', 'bit', 'research', 'trying', 'go', 'ahead', 'aunty', 'topic']\n",
      "First 10 Stemmed Tokens: ['hi', 'good', 'tell', 'bit', 'research', 'tri', 'go', 'ahead', 'aunti', 'topic']\n",
      "Sentiment: Sentiment(polarity=0.21374631268436578, subjectivity=0.46556992663187335)\n",
      "First 10 Noun Phrases: ['hi', \"'m good\", 'yeah', 'yeah', 'question mark', 'generate fraternity', 'wrong person', 'question mark', 'okay', 'yeah']\n",
      "Top 10 Word Frequencies: [('not', 28), ('know', 18), ('people', 17), ('no', 15), ('now', 14), ('think', 14), ('person', 13), ('feel', 11), ('able', 11), ('make', 10)]\n",
      "Dominant Topic: 2\n",
      "Word-Weight Pairs:\n",
      "know, 0.02800000086426735\n",
      "not, 0.020999999716877937\n",
      "think, 0.017000000923871994\n",
      "people, 0.01600000075995922\n",
      "feel, 0.009999999776482582\n",
      "say, 0.00800000037997961\n",
      "one, 0.00800000037997961\n",
      "now, 0.007000000216066837\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#id 8 Shveta\n",
    "get_individual_details(\"shveta\", tokenized_data, dominant_topic_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "26162b96-3a27-4770-b66d-04a53c6f9c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details for Ranjana:\n",
      "First 10 Word Tokens: ['many', 'times', 'attended', 'goals', 'attended', 'threeday', 'course', 'last', 'session', 'november']\n",
      "First 10 Stemmed Tokens: ['mani', 'time', 'attend', 'goal', 'attend', 'threeday', 'cours', 'last', 'session', 'novemb']\n",
      "Sentiment: Sentiment(polarity=0.15973224306557637, subjectivity=0.4555228721895389)\n",
      "First 10 Noun Phrases: ['vipassana', 'ranjanaa', 'three-day course', 'november', 'main goals', 'mental peace', \"life 's\", 'difficult questions', 'fortunately', 'own actions']\n",
      "Top 10 Word Frequencies: [('practice', 8), ('understanding', 6), ('morality', 5), ('not', 5), ('rather', 5), ('community', 5), ('practices', 5), ('social', 5), ('life', 4), ('teaches', 4)]\n",
      "Dominant Topic: 3\n",
      "Word-Weight Pairs:\n",
      "practice, 0.012000000104308128\n",
      "not, 0.008999999612569809\n",
      "sense, 0.007000000216066837\n",
      "understanding, 0.007000000216066837\n",
      "life, 0.007000000216066837\n",
      "might, 0.006000000052154064\n",
      "change, 0.006000000052154064\n",
      "self, 0.006000000052154064\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#id 9 Ranjana\n",
    "get_individual_details(\"ranjana\", tokenized_data, dominant_topic_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "770eaef4-8854-46ba-a6af-56f487638e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details for Puneet:\n",
      "First 10 Word Tokens: ['good', 'afternoon', 'puneet', 'teacher', 'someone', 'experienced', 'various', 'facets', 'share', 'unique']\n",
      "First 10 Stemmed Tokens: ['good', 'afternoon', 'puneet', 'teacher', 'someon', 'experienc', 'variou', 'facet', 'share', 'uniqu']\n",
      "Sentiment: Sentiment(polarity=0.11257816257816258, subjectivity=0.5751202501202499)\n",
      "First 10 Noun Phrases: ['good afternoon', 'puneet', 'vipassana', 'various facets', 'unique perspective', 'good afternoon', 'vipassana', 'vipassana', 'vipassana', '’ s']\n",
      "Top 10 Word Frequencies: [('practice', 9), ('sometimes', 7), ('teachings', 7), ('often', 6), ('approach', 6), ('view', 5), ('personal', 5), ('simplicity', 5), ('practices', 5), ('living', 5)]\n",
      "Dominant Topic: 1\n",
      "Word-Weight Pairs:\n",
      "practice, 0.008999999612569809\n",
      "meditation, 0.008999999612569809\n",
      "nature, 0.007000000216066837\n",
      "often, 0.006000000052154064\n",
      "understanding, 0.006000000052154064\n",
      "law, 0.004999999888241291\n",
      "life, 0.004999999888241291\n",
      "experience, 0.004999999888241291\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#id 10 puneet\n",
    "get_individual_details(\"puneet\", tokenized_data, dominant_topic_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b67c05d3-b533-44b6-a115-ce33377c4f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details for Tonu:\n",
      "First 10 Word Tokens: ['tell', 'bit', 'research', 'proceed', 'gone', 'last', 'year', 'master', 'right', 'now']\n",
      "First 10 Stemmed Tokens: ['tell', 'bit', 'research', 'proceed', 'gone', 'last', 'year', 'master', 'right', 'now']\n",
      "Sentiment: Sentiment(polarity=0.1847653549148113, subjectivity=0.4958143140072488)\n",
      "First 10 Noun Phrases: ['vipassana', \"master 's\", 'tiss', 'development studies', 'credit dissertation', 'big chunk', 'vipassana', 'question mark', 'main idea', 'generate fraternity']\n",
      "Top 10 Word Frequencies: [('know', 85), ('not', 54), ('people', 46), ('think', 40), ('day', 33), ('maybe', 29), ('feel', 26), ('practice', 26), ('time', 24), ('go', 24)]\n",
      "Dominant Topic: 2\n",
      "Word-Weight Pairs:\n",
      "know, 0.02800000086426735\n",
      "not, 0.020999999716877937\n",
      "think, 0.017000000923871994\n",
      "people, 0.01600000075995922\n",
      "feel, 0.009999999776482582\n",
      "say, 0.00800000037997961\n",
      "one, 0.00800000037997961\n",
      "now, 0.007000000216066837\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#id 11 Tonu\n",
    "get_individual_details(\"tonu\", tokenized_data, dominant_topic_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6f4a0947-a03e-4684-bd44-ff733867d51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details for Anirudh:\n",
      "First 10 Word Tokens: ['initially', 'motivated', 'try', 'practicing', 'mindfulness', 'center', 'beneficial', 'seemed', 'natural', 'progression']\n",
      "First 10 Stemmed Tokens: ['initi', 'motiv', 'tri', 'practic', 'mind', 'center', 'benefici', 'seem', 'natur', 'progress']\n",
      "Sentiment: Sentiment(polarity=0.15703578336557056, subjectivity=0.45006102053974384)\n",
      "First 10 Noun Phrases: ['vipassana', 'anirudh narula', 'vipassana', 'natural progression', 'vipassana', 'anirudh narula', 'internal processes', 'vipassana', \"'m okay\", 'external circumstances']\n",
      "Top 10 Word Frequencies: [('personal', 7), ('practice', 6), ('life', 6), ('understanding', 5), ('not', 5), ('volunteering', 5), ('challenges', 5), ('increased', 4), ('however', 4), ('maitri', 4)]\n",
      "Dominant Topic: 0\n",
      "Word-Weight Pairs:\n",
      "life, 0.008999999612569809\n",
      "personal, 0.00800000037997961\n",
      "experience, 0.007000000216066837\n",
      "practice, 0.006000000052154064\n",
      "think, 0.004999999888241291\n",
      "issues, 0.004999999888241291\n",
      "societal, 0.004999999888241291\n",
      "understanding, 0.004999999888241291\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#id 12 Anirudh\n",
    "get_individual_details(\"anirudh\", tokenized_data, dominant_topic_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2fce0659-9a95-4c64-8b6d-5b680207f6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details for Nandkishore:\n",
      "First 10 Word Tokens: ['nandkishore', 'describe', 'experience', 'meditation', 'everything', 'seems', 'awakened', 'meditating', 'long', 'time']\n",
      "First 10 Stemmed Tokens: ['nandkishor', 'describ', 'experi', 'medit', 'everyth', 'seem', 'awaken', 'medit', 'long', 'time']\n",
      "Sentiment: Sentiment(polarity=0.08431961120640363, subjectivity=0.40599689618557533)\n",
      "First 10 Noun Phrases: ['nandkishore', 'vipassana', 'long time', 'family members', 'meditating', 'ripple effect', 'modern lifestyles', 'consumption patterns', \"n't align\", 'natural processes']\n",
      "Top 10 Word Frequencies: [('meditation', 16), ('spiritual', 11), ('societal', 11), ('development', 8), ('nature', 7), ('understanding', 7), ('practice', 6), ('often', 5), ('sense', 5), ('see', 5)]\n",
      "Dominant Topic: 1\n",
      "Word-Weight Pairs:\n",
      "practice, 0.008999999612569809\n",
      "meditation, 0.008999999612569809\n",
      "nature, 0.007000000216066837\n",
      "often, 0.006000000052154064\n",
      "understanding, 0.006000000052154064\n",
      "law, 0.004999999888241291\n",
      "life, 0.004999999888241291\n",
      "experience, 0.004999999888241291\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#id 13 Nandkishore\n",
    "get_individual_details(\"nandkishore\", tokenized_data, dominant_topic_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "44cea310-e1fa-4c2a-9d5f-16d4574cada9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details for Premchand:\n",
      "First 10 Word Tokens: ['mental', 'state', 'influenced', 'practicing', 'since', 'started', 'practicing', 'noticeable', 'shift', 'mental']\n",
      "First 10 Stemmed Tokens: ['mental', 'state', 'influenc', 'practic', 'sinc', 'start', 'practic', 'notic', 'shift', 'mental']\n",
      "Sentiment: Sentiment(polarity=0.12624999999999997, subjectivity=0.45500000000000007)\n",
      "First 10 Noun Phrases: ['mental state', 'vipassana', 'vipassana', 'definitely', 'healthy way', 'conscious choice', 'external factors disturb', 'inner peace', 'post-vipassana', 'interestingly']\n",
      "Top 10 Word Frequencies: [('practice', 5), ('others', 5), ('maitri', 4), ('change', 4), ('practicing', 3), ('not', 3), ('life', 3), ('sense', 3), ('mental', 2), ('since', 2)]\n",
      "Dominant Topic: 3\n",
      "Word-Weight Pairs:\n",
      "practice, 0.012000000104308128\n",
      "not, 0.008999999612569809\n",
      "sense, 0.007000000216066837\n",
      "understanding, 0.007000000216066837\n",
      "life, 0.007000000216066837\n",
      "might, 0.006000000052154064\n",
      "change, 0.006000000052154064\n",
      "self, 0.006000000052154064\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#id 14 Premchand\n",
    "get_individual_details(\"premchand\", tokenized_data, dominant_topic_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c422e2ac-0f83-4ff1-a7a2-e6669c1c17e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details for Jyoti:\n",
      "First 10 Word Tokens: ['describe', 'meditation', 'changed', 'perception', 'actions', 'absolutely', 'began', 'practicing', 'sort', 'blindness']\n",
      "First 10 Stemmed Tokens: ['describ', 'medit', 'chang', 'percept', 'action', 'absolut', 'began', 'practic', 'sort', 'blind']\n",
      "Sentiment: Sentiment(polarity=0.08313615608136156, subjectivity=0.427264326236929)\n",
      "First 10 Noun Phrases: ['vipassana', 'absolutely', 'vipassana', 'own self', 'various situations', 'significant realization', 'inner calmness', 'have', 'specific changes', 'day-to-day life']\n",
      "Top 10 Word Frequencies: [('not', 7), ('now', 7), ('sense', 6), ('people', 6), ('towards', 5), ('feel', 5), ('self', 4), ('empathy', 4), ('service', 4), ('others', 4)]\n",
      "Dominant Topic: 3\n",
      "Word-Weight Pairs:\n",
      "practice, 0.012000000104308128\n",
      "not, 0.008999999612569809\n",
      "sense, 0.007000000216066837\n",
      "understanding, 0.007000000216066837\n",
      "life, 0.007000000216066837\n",
      "might, 0.006000000052154064\n",
      "change, 0.006000000052154064\n",
      "self, 0.006000000052154064\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#id 15 Jyoti\n",
    "get_individual_details(\"jyoti\", tokenized_data, dominant_topic_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1f5be17c-f847-4558-b2f6-5dfa652e0ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details for Sushila:\n",
      "First 10 Word Tokens: ['describe', 'changes', 'experienced', 'since', 'practicing', 'transformative', 'experience', 'change', 'certain', 'aspects']\n",
      "First 10 Stemmed Tokens: ['describ', 'chang', 'experienc', 'sinc', 'practic', 'transform', 'experi', 'chang', 'certain', 'aspect']\n",
      "Sentiment: Sentiment(polarity=0.10619182900432897, subjectivity=0.4549312839937839)\n",
      "First 10 Noun Phrases: ['vipassana', 'vipassana', 'transformative experience', 'certain aspects', 'new way', \"'pure birth\", 'vipassana', 'regular practice', 'vipassana', 'haven ’ t']\n",
      "Top 10 Word Frequencies: [('practice', 11), ('maitri', 7), ('social', 6), ('life', 5), ('address', 5), ('issues', 5), ('change', 4), ('way', 4), ('people', 4), ('not', 4)]\n",
      "Dominant Topic: 3\n",
      "Word-Weight Pairs:\n",
      "practice, 0.012000000104308128\n",
      "not, 0.008999999612569809\n",
      "sense, 0.007000000216066837\n",
      "understanding, 0.007000000216066837\n",
      "life, 0.007000000216066837\n",
      "might, 0.006000000052154064\n",
      "change, 0.006000000052154064\n",
      "self, 0.006000000052154064\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#id 16 Sushila\n",
    "get_individual_details(\"sushila\", tokenized_data, dominant_topic_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "27b66ee9-7b2c-4faf-8fdd-81f07766e4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details for Himmat:\n",
      "First 10 Word Tokens: ['perspective', 'mindset', 'change', 'practicing', 'postvipassana', 'noticed', 'significant', 'shift', 'towards', 'positive']\n",
      "First 10 Stemmed Tokens: ['perspect', 'mindset', 'chang', 'practic', 'postvipassana', 'notic', 'signific', 'shift', 'toward', 'posit']\n",
      "Sentiment: Sentiment(polarity=0.14952161365204847, subjectivity=0.48889924085576264)\n",
      "First 10 Noun Phrases: ['vipassana', 'post-vipassana', 'positive thoughts', 'new dimension', 'vipassana', 'don ’ t practice', 'vipassana', 'vipassana', 'personal relationships', 'various factors interplay']\n",
      "Top 10 Word Frequencies: [('life', 9), ('experience', 8), ('think', 7), ('change', 6), ('personal', 5), ('societal', 5), ('peace', 5), ('daily', 4), ('relationships', 4), ('no', 4)]\n",
      "Dominant Topic: 0\n",
      "Word-Weight Pairs:\n",
      "life, 0.008999999612569809\n",
      "personal, 0.00800000037997961\n",
      "experience, 0.007000000216066837\n",
      "practice, 0.006000000052154064\n",
      "think, 0.004999999888241291\n",
      "issues, 0.004999999888241291\n",
      "societal, 0.004999999888241291\n",
      "understanding, 0.004999999888241291\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#id 17 Himmat\n",
    "get_individual_details(\"himmat\", tokenized_data, dominant_topic_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "167fc082-16f0-461f-8a67-f69ebb936683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details for Fgd1:\n",
      "First 10 Word Tokens: ['let', 'begin', 'discussion', 'see', 'fraternity', 'context', 'caste', 'class', 'society', 'fraternity']\n",
      "First 10 Stemmed Tokens: ['let', 'begin', 'discuss', 'see', 'fratern', 'context', 'cast', 'class', 'societi', 'fratern']\n",
      "Sentiment: Sentiment(polarity=0.058017868656166545, subjectivity=0.4219812102790827)\n",
      "First 10 Noun Phrases: ['fraternity', 'own caste', 'caste', 'significant factor', 'vipassana', 'empower individuals', 'vipassana', 'isn ’ t', 'natural law', 'have']\n",
      "Top 10 Word Frequencies: [('self', 16), ('foucault', 9), ('way', 8), ('thoughts', 6), ('understanding', 5), ('practice', 5), ('sensations', 5), ('context', 4), ('others', 4), ('see', 3)]\n",
      "Dominant Topic: 3\n",
      "Word-Weight Pairs:\n",
      "practice, 0.012000000104308128\n",
      "not, 0.008999999612569809\n",
      "sense, 0.007000000216066837\n",
      "understanding, 0.007000000216066837\n",
      "life, 0.007000000216066837\n",
      "might, 0.006000000052154064\n",
      "change, 0.006000000052154064\n",
      "self, 0.006000000052154064\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#id 18 fgd1\n",
    "get_individual_details(\"fgd1\", tokenized_data, dominant_topic_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b4e52c36-9b6b-43ea-9072-1218ee100375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details for Fgd2:\n",
      "First 10 Word Tokens: ['today', 'gathered', 'diverse', 'group', 'dive', 'discussion', 'let', 'start', 'personal', 'perspective']\n",
      "First 10 Stemmed Tokens: ['today', 'gather', 'divers', 'group', 'dive', 'discuss', 'let', 'start', 'person', 'perspect']\n",
      "Sentiment: Sentiment(polarity=0.118826705940108, subjectivity=0.4247259041073473)\n",
      "First 10 Noun Phrases: ['diverse group', 'vipassana', '’ s', 'personal perspective', 'ritu', 'vipassana', 'mental health', 'vipassana', 'transformative experience', 'mental health']\n",
      "Top 10 Word Frequencies: [('energy', 13), ('quantum', 11), ('practice', 10), ('mental', 7), ('physics', 6), ('scientific', 5), ('health', 4), ('meditation', 4), ('way', 4), ('stress', 4)]\n",
      "Dominant Topic: 3\n",
      "Word-Weight Pairs:\n",
      "practice, 0.012000000104308128\n",
      "not, 0.008999999612569809\n",
      "sense, 0.007000000216066837\n",
      "understanding, 0.007000000216066837\n",
      "life, 0.007000000216066837\n",
      "might, 0.006000000052154064\n",
      "change, 0.006000000052154064\n",
      "self, 0.006000000052154064\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#id 19 fgd2\n",
    "get_individual_details(\"fgd2\", tokenized_data, dominant_topic_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5935926-de89-4526-b078-22627a7204c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
